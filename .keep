import torch

def predict_series_of_tokens(model, context, tokenizer, num_tokens=10, top_k=5, top_p=0.9, temperature=1.5, max_context_size=50):
    """
    Predict a series of tokens based on the initial context using the model.

    Args:
        model: The trained Transformer model.
        context: Initial text context as a string.
        tokenizer: Tokenizer used for encoding and decoding text.
        num_tokens: Number of tokens to predict in the series.
        top_k: Number of top predictions to consider.
        top_p: Cumulative probability threshold for nucleus sampling.
        temperature: Controls the randomness of predictions by scaling the logits.
        max_context_size: Maximum number of tokens allowed in the context.

    Returns:
        A list of predicted tokens and their corresponding decoded text.
    """
    # Encode the initial context and extract token IDs
    context_encoded = tokenizer.encode(context)
    context_ids = context_encoded.ids

    predicted_tokens = []
    decoded_text = context

    for _ in range(num_tokens):
        # Ensure context length does not exceed max_context_size
        if len(context_ids) > max_context_size:
            context_ids.pop(0)  # Remove the oldest token
        
        # Update positional encoding and mask
        input_tensor = torch.tensor([context_ids]).to(DEVICE)
        mask = look_ahead_mask(input_tensor.size(1)).to(DEVICE)
        
        # Predict next token
        values, indices = predict_next_token(model, context_ids, top_k, temperature)
        
        # Apply temperature scaling directly to logits before softmax
        scaled_logits = values / temperature
        
        # Clamp logits to avoid extreme values and numerical instability
        scaled_logits = torch.clamp(scaled_logits, min=-1e10, max=1e10)
        
        # Apply softmax to get probabilities
        probabilities = torch.softmax(scaled_logits, dim=-1)
        
        # Debug: Print probabilities and logits
        print(f"Logits: {values}")
        print(f"Scaled Logits: {scaled_logits}")
        print(f"Probabilities: {probabilities}")

        # Implement top-p (nucleus) sampling
        sorted_probs, sorted_indices = torch.sort(probabilities, descending=True)
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
        sorted_indices_to_keep = cumulative_probs < top_p
        sorted_indices_to_keep = sorted_indices_to_keep | (cumulative_probs == cumulative_probs[0])  # Always keep at least one token
        
        filtered_probs = sorted_probs[sorted_indices_to_keep]
        filtered_indices = sorted_indices[sorted_indices_to_keep]
        
        # Re-normalize filtered probabilities
        filtered_probs = filtered_probs / torch.sum(filtered_probs)
        
        # Check if filtered_probs has enough tokens; if not, fallback to deterministic choice
        if len(filtered_probs) == 0:
            # Fallback to the highest probability token
            next_token = sorted_indices.view(-1)[0].item()
        else:
            # Randomly select a token from the filtered list
            next_token_index = torch.multinomial(filtered_probs, 1).item()
            next_token = filtered_indices[next_token_index].item()  # Map back to original token indices
        
        # Decode the predicted token
        decoded_token = tokenizer.decode([next_token])
        print(f"Predicted Token ID: {next_token}, Decoded Token: '{decoded_token}'")  # Debug: print the decoded token
        
        # Append token and adjust context
        context_ids.append(next_token)
        decoded_text += decoded_token

        # Add the predicted token to the list
        predicted_tokens.append((next_token, decoded_token))

        # Break if end token is reached
        if decoded_token.strip() == "</s>":
            break

    return predicted_tokens, decoded_text

# Example usage
context = "The quick brown fox jumps over the lazy dog."
predicted_tokens, full_text = predict_series_of_tokens(model, context, tokenizer, num_tokens=10, top_k=10, top_p=0.4, temperature=1.2)

# Print results
for token_id, token_text in predicted_tokens:
    print(f"Predicted Token ID: {token_id}, Predicted Token: '{token_text}'")

print("\nFull Predicted Text:", full_text)
