{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the initial dataset which is used in the first initial step of training after this the model should be able to complete text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from preprocess.sequencing import create_sequences\n",
    "from preprocess.tokenizer import BPETokenizer\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 5742\n",
      "Validation set size: 267\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SPECIAL_TOKENS = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
    "TEXT_COMPLETION_PATH = os.path.join(\"data\", \"text_completion.json\")\n",
    "\n",
    "train_set = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"train[:2%]\")\n",
    "\n",
    "# Load 5% of the validation set\n",
    "valid_set = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"validation[:2%]\")\n",
    "\n",
    "print(f\"Training set size: {len(train_set)}\")\n",
    "print(f\"Validation set size: {len(valid_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles = train_set[\"article\"]\n",
    "train_highlights = train_set[\"highlights\"]\n",
    "\n",
    "\n",
    "tokenizer = BPETokenizer(\n",
    "    vocab_size=30000, min_frequency=2, special_tokens=SPECIAL_TOKENS\n",
    ")\n",
    "\n",
    "if not os.path.exists(TEXT_COMPLETION_PATH):\n",
    "    tokenizer.fit(\n",
    "        train_articles + train_highlights,\n",
    "    )\n",
    "    tokenizer.save(TEXT_COMPLETION_PATH)\n",
    "else:\n",
    "    tokenizer.load(TEXT_COMPLETION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Train Articles: 100%|██████████| 5742/5742 [00:00<00:00, 9918.67it/s] \n",
      "Extracting Valid Articles: 100%|██████████| 267/267 [00:00<00:00, 4895.65it/s]\n",
      "Encoding Train Set: 100%|██████████| 5742/5742 [00:07<00:00, 731.21it/s]\n",
      "Encoding Valid Set: 100%|██████████| 267/267 [00:00<00:00, 737.61it/s]\n"
     ]
    }
   ],
   "source": [
    "train_articles = [item[\"article\"] for item in tqdm(train_set, desc=\"Extracting Train Articles\") if item[\"article\"] is not None]\n",
    "valid_articles = [item[\"article\"] for item in tqdm(valid_set, desc=\"Extracting Valid Articles\") if item[\"article\"] is not None]\n",
    "\n",
    "def encode_article(article):\n",
    "    return tokenizer.encode(article)\n",
    "\n",
    "def parallel_encode(articles, desc):\n",
    "    encoded_articles = []\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = {executor.submit(encode_article, article): article for article in articles}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=desc):\n",
    "            encoded_articles.append(future.result())\n",
    "    return encoded_articles\n",
    "\n",
    "train_set_encoded = parallel_encode(train_articles, \"Encoding Train Set\")\n",
    "valid_set_encoded = parallel_encode(valid_articles, \"Encoding Valid Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Token IDs: 100%|██████████| 5742/5742 [00:00<00:00, 15342.94it/s]\n",
      "Extracting Token IDs: 100%|██████████| 267/267 [00:00<00:00, 13422.98it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_token_ids(encoded_data):\n",
    "    \"\"\"\n",
    "    Convert each Encoding object into its list of token IDs and flatten them into a single list,\n",
    "    with a progress bar showing the extraction progress.\n",
    "    \"\"\"\n",
    "    flattened_ids = []\n",
    "    for encoding in tqdm(encoded_data, desc=\"Extracting Token IDs\"):\n",
    "        flattened_ids.extend(encoding.ids)\n",
    "    return flattened_ids\n",
    "\n",
    "# Extract token IDs with progress bars for training and validation sets\n",
    "train_token_ids = extract_token_ids(train_set_encoded)\n",
    "valid_token_ids = extract_token_ids(valid_set_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_ELN = 50 # N\n",
    "TARGET_ELN = 1\n",
    "\n",
    "train_seq = create_sequences(\n",
    "    tokenized_data=train_token_ids, \n",
    "    max_context_length=CONTEXT_ELN,\n",
    "    max_target_length=TARGET_ELN,\n",
    "    skip_processed=True,\n",
    ")\n",
    "\n",
    "valid_seq = create_sequences(\n",
    "    tokenized_data=valid_token_ids,\n",
    "    max_context_length=CONTEXT_ELN,\n",
    "    max_target_length=TARGET_ELN,\n",
    "    skip_processed=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91024\n",
      "Context: [20604, 16, 1354, 467, 9296, 8931, 13, 596, 3949, 11831]... (Total: 50 tokens)\n",
      "Target: [306] (Total: 1 token)\n",
      "Decoded: ... Radcliffe\n",
      "Decoded:  as\n",
      "Context: [306, 3949, 11831, 231, 366, 22955, 11831, 240, 210, 15654]... (Total: 50 tokens)\n",
      "Target: [45] (Total: 1 token)\n",
      "Decoded: ...parties. \"\n",
      "Decoded: I\n",
      "Context: [45, 894, 519, 961, 229, 250, 493, 237, 964, 550]... (Total: 50 tokens)\n",
      "Target: [18] (Total: 1 token)\n",
      "Decoded: ...xtravagant\n",
      "Decoded: .\n",
      "Context: [18, 366, 526, 1771, 281, 638, 6310, 352, 1771, 289]... (Total: 50 tokens)\n",
      "Target: [30] (Total: 1 token)\n",
      "Decoded: ...lm \"Hostel\n",
      "Decoded: :\n",
      "Context: [30, 3254, 4627, 557, 2579, 1232, 4182, 4362, 317, 1334]... (Total: 50 tokens)\n",
      "Target: [237] (Total: 1 token)\n",
      "Decoded: ... some sort\n",
      "Decoded:  of\n",
      "Context: [237, 1795, 557, 252, 311, 231, 309, 2020, 18, 366]... (Total: 50 tokens)\n",
      "Target: [9578] (Total: 1 token)\n",
      "Decoded: ...g fame and\n",
      "Decoded:  ric\n"
     ]
    }
   ],
   "source": [
    "print(len(train_seq))\n",
    "for i, (context, target) in enumerate(train_seq):\n",
    "    print(f\"Context: {context[:10]}... (Total: {len(context)} tokens)\") \n",
    "    print(f\"Target: {target} (Total: {len(target)} token)\") \n",
    "    print(f\"Decoded: ...{tokenizer.decode(context)[-10:]}\")\n",
    "    print(f\"Decoded: {tokenizer.decode(target)}\")\n",
    "    if i == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCompletionDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.sequences[idx]\n",
    "        # Convert context and target to tensors\n",
    "        context = torch.tensor(context, dtype=torch.long)\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "        return context, target\n",
    "\n",
    "    \n",
    "train_dataset = TextCompletionDataset(train_seq)\n",
    "valid_dataset = TextCompletionDataset(valid_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "VOC_SIZE = tokenizer.get_vocab_size()\n",
    "MAX_LEN = CONTEXT_ELN\n",
    "D_MODEL = 512\n",
    "FFN_HIDDEN = 2048\n",
    "N_HEAD = 8\n",
    "N_LAYERS = 6\n",
    "DROP_PROB = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set - Total Elements: 4642224\n",
      "NaN Values: 0\n",
      "Infinite Values: 0\n",
      "Unstable Values: 0\n",
      "Validation Set - Total Elements: 193545\n",
      "NaN Values: 0\n",
      "Infinite Values: 0\n",
      "Unstable Values: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Function to check for NaN, infinite, or unstable values in datasets\n",
    "def check_data_stability(data_loader, data_name=\"Dataset\"):\n",
    "    nan_count = 0\n",
    "    inf_count = 0\n",
    "    unstable_count = 0\n",
    "    total_elements = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        context, target = batch\n",
    "\n",
    "        # Convert tensors to numpy for detailed checks\n",
    "        context_np = context.numpy()\n",
    "        target_np = target.numpy()\n",
    "\n",
    "        # Check for NaN values\n",
    "        nan_count += np.isnan(context_np).sum() + np.isnan(target_np).sum()\n",
    "\n",
    "        # Check for infinite values\n",
    "        inf_count += np.isinf(context_np).sum() + np.isinf(target_np).sum()\n",
    "\n",
    "        # Check for unstable values (very large or very small values)\n",
    "        unstable_count += np.sum((np.abs(context_np) > 1e6) | (np.abs(context_np) < 1e-6))\n",
    "\n",
    "        total_elements += context_np.size + target_np.size\n",
    "\n",
    "    print(f\"{data_name} - Total Elements: {total_elements}\")\n",
    "    print(f\"NaN Values: {nan_count}\")\n",
    "    print(f\"Infinite Values: {inf_count}\")\n",
    "    print(f\"Unstable Values: {unstable_count}\")\n",
    "\n",
    "# Check the training and validation datasets for stability\n",
    "check_data_stability(train_loader, \"Training Set\")\n",
    "check_data_stability(valid_loader, \"Validation Set\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
